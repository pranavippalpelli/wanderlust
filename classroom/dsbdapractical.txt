#-----------------------------------------------------------------------------------------------------------------------------------------------------------
Practical No: 1
Data Wrangling, I 
Perform the following operations using Python on any open-source dataset (e.g., data.csv) 
1. Import all the required Python Libraries. 
2. Locate an open-source data from the web (e.g., https://www.kaggle.com). 
Provide a clear description of the data and its source (i.e., URL of the web site). 
3. Load the Dataset into pandas dataframe. 
4. Data Preprocessing: check for missing values in the data using pandas isnull(), describe() function to get some initial statistics. Provide variable descriptions. Types of variables etc. Check the dimensions of the data frame. 
5. Data Formatting and Data Normalization: Summarize the types of variables by checking the data types (i.e., character, numeric, integer, factor, and logical) of the variables in the data set. If variables are not in the correct data type, apply proper type conversions. 
6. Turn categorical variables into quantitative variables in Python. 

#-----------------------------------------------------------------------------------------------------------------------------------------------------------

import pandas as pd
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv("StudentsPerformance.csv")

df.isnull()
df.isnull().sum()
df.describe()

df.shape
df.dtypes

df['writing score'] = df['writing score'].astype(float)   #not necessary to convert writing score from int to float but as in practicle there is 1 query to 								show this conversion so i do it
df.info()

le = LabelEncoder()
df['gender'] = le.fit_transform(df['gender'])

le = LabelEncoder()
df['race/ethnicity'] = le.fit_transform(df['race/ethnicity'])

le = LabelEncoder()
df['parental level of education'] = le.fit_transform(df['parental level of education'])

le = LabelEncoder()
df['lunch'] = le.fit_transform(df['lunch'])

le = LabelEncoder()
df['test preparation course'] = le.fit_transform(df['test preparation course'])

df.info()

#-----------------------------------------------------------------------------------------------------------------------------------------------------------
Practical No: 2
Data Wrangling II 
Create an “Academic performance” dataset of students and perform the following operations using Python. 
1. Scan all variables for missing values and inconsistencies. If there are missing values and/or inconsistencies, use any of the suitable techniques to deal with them. 
2. Scan all numeric variables for outliers. If there are outliers, use any of the suitable techniques to deal with them. 
3. Apply data transformations on at least one of the variables. The purpose of this transformation should be one of the following reasons: to change the scale for better understanding of the variable, to convert a non-linear relation into a linear one, or to decrease the skewness and convert the distribution into a normal distribution. 


#-----------------------------------------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np

data = {
    'Student_id':[ 1, 2, 3, 4, 5, 6 ],
    'Math_Score':[ 85, 90, 100, 40, 200, 95],
    'English_Score':[ 78, 82, 75, 60, 58, None],
    'Science_Score':[ 88, 92, 80, np.nan, 30, 100],
    'Attendance':[ 95, 80, 85, 50, 110, 88]
}

df = pd.DataFrame(data)
df.to_csv("AcademicPerformance.csv", index = False)
df = pd.read_csv("AcademicPerformance.csv")

df.isnull().sum()

english_mean = df['English_Score'].mean()
df['English_Score'] = df['English_Score'].fillna(english_mean)
science_mean = df['Science_Score'].mean()
df['Science_Score'] = df['Science_Score'].fillna(science_mean)


#outlier for mathscore-

Q1 = df['Math_Score'].quantile(0.25)
print(Q1)
Q3 = df['Math_Score'].quantile(0.75)

IQR = Q3 - Q1

lower_limit = Q1 - 1.5 *IQR
upper_limit = Q3 + 1.5 *IQR

df['Math_Score'] = df['Math_Score'].clip(lower_limit, upper_limit)


#for sciencescore
Q1 = df['Science_Score'].quantile(0.25)
Q3 = df['Science_Score'].quantile(0.75)
IQR = Q3 - Q1
lower_limit = Q1 - 1.5 *IQR
upper_limit = Q3 + 1.5 *IQR
df['Science_Score'] = df['Science_Score'].clip(lower_limit, upper_limit)

#for attendance
Q1 = df['Attendance'].quantile(0.25)
Q3 = df['Attendance'].quantile(0.75)
IQR = Q3 - Q1
lower_limit = Q1 - 1.5 *IQR
upper_limit = Q3 + 1.5 *IQR
df['Attendance'] = df['Attendance'].clip(lower_limit, upper_limit)


df.boxplot()

#handles log:-

df['Math_Score_Log'] = np.log(df['Math_Score'])  # log1p handles log(0)
df


#-----------------------------------------------------------------------------------------------------------------------------------------------------------
PRACTICAL NO:3
Data Wrangling 
Create an “students_data.csv” and perform the following operations using Python
1.	Load the file students_data.csv into a Pandas DataFrame.
2.	Display the first 5 rows of the data.
3.	How many rows and columns are there in the dataset?
4.	List all the column names.
5.	Check if there are any missing values in the dataset.
6.	Replace missing values in the "Marks" column with the average marks.
7.	Select all the rows where students scored more than 80 marks.

#-----------------------------------------------------------------------------------------------------------------------------------------------------------


# Step 1: Create the students_data.csv file (optional if you're generating it);
import pandas as pd

# Sample data dictionary
data = {
    "Name": ["Alice", "Bob", "Charlie", "David", "Eve", "Frank", "Grace", "Heidi"],
    "Age": [20, 21, 22, 20, 23, 21, 22, 24],
    "Marks": [85, 78, None, 92, 67, 88, None, 95]
}

# Create DataFrame and save to CSV
df = pd.DataFrame(data)
df.to_csv("students_data.csv", index=False)


# 1. Load the CSV file
df = pd.read_csv("students_data.csv")

# 2. Display the first 5 rows
print("First 5 rows:")
df.head()

# 3. Get the number of rows and columns
rows, cols = df.shape
print(f"\nTotal Rows: {rows}, Total Columns: {cols}")

# 4. List all column names
print("\nColumn Names:")
df.columns.tolist()

# 5. Check for missing values
print("\nMissing Values:")
df.isnull().sum()

# 6. Replace missing values in "Marks" with average
average_marks = df["Marks"].mean()
df["Marks"] = df["Marks"].fillna(average_marks)
print("\nMissing values in 'Marks' column replaced with average.")

# 7. Select rows where Marks > 80
high_scorers = df[df["Marks"] > 80]
print("\nStudents with Marks > 80:")
print(high_scorers)





#-----------------------------------------------------------------------------------------------------------------------------------------------------------
PRACTICAL N0:4:
Descriptive Statistics - Measures of Central Tendency and variability 
Perform the following operations on any open-source dataset (e.g., data.csv) 
1. Provide summary statistics (mean, median, minimum, maximum, standard deviation) for a dataset (age, income etc.) with numeric variables grouped by one of the qualitative (categorical) variable. For example, if your categorical variable is age groups and quantitative variable is income, then provide summary statistics of income grouped by the age groups. Create a list that contains a numeric value for each response to the categorical variable. 
2. Write a Python program to display some basic statistical details like percentile, mean, standard deviation etc. of the species of ‘Iris-setosa’, ‘Iris-versicolor’ and ‘Iris-versicolor’ of iris.csv dataset. 

#-----------------------------------------------------------------------------------------------------------------------------------------------------------






# 1: Summary statistics grouped by a categorical variable
import pandas as pd

# Load Iris dataset
df = pd.read_csv("https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv")

# Grouped summary statistics for 'sepal_length' by 'species'
grouped_stats = df.groupby('species')['sepal_length'].agg(['mean', 'median', 'min', 'max', 'std'])
print("Summary statistics for 'sepal_length' grouped by 'species':\n")
print(grouped_stats)

# Create a list with sepal_length values for each species
grouped_list = df.groupby('species')['sepal_length'].apply(list)
print("\nList of sepal_length values for each species:\n")
print(grouped_list)


# 2: Statistical details for each species
import numpy as np


sentosa=df[df['species']=='setosa']
sentosa.describe()

versicolor =df[df['species'] == 'versicolor']
versicolor.describe()

virginica = df[df['species'] == 'virginica']
virginica.describe()




#-----------------------------------------------------------------------------------------------------------------------------------------------------------
Practical 5:
Data Analytics I 
Create a Linear Regression Model using Python/R to predict home prices using Boston Housing Dataset (https://www.kaggle.com/c/boston-housing). The Boston Housing dataset contains information about various houses in Boston through different parameters. There are 506 samples and 14 feature variables in this dataset. 

#-----------------------------------------------------------------------------------------------------------------------------------------------------------


import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error


# Load the dataset
df = pd.read_csv("/content/HousingData.csv")

# Display column names (optional)
print("Column Names:", df.columns)

# Define independent (X) and dependent (y) variables

cleandata = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age',
             'dis', 'rad', 'tax', 'ptratio', 'black', 'lstat', 'medv']

for col in cleandata:
	Q1 = df[col].quantile(0.25)
	Q3 = df[col].quantile(0.75)
	IQR = Q3 - Q1
	lower_limit = Q1 - 1.5 *IQR
	upper_limit = Q3 + 1.5 *IQR
	df[col] = df[col].clip(lower_limit, upper_limit)


x = df[['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age',
        'dis', 'rad', 'tax', 'ptratio', 'black', 'lstat','medv']]
y = df[['medv']]  # MEDV is the median house value in $1000s

# Check for missing values and fill if necessary
x = x.fillna(x.mean())
y = y.fillna(y.mean())

# Split into training and test sets (75% train, 25% test)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# Create and train the Linear Regression model
model = LinearRegression()
model.fit(x_train, y_train)

# Make predictions on test set
y_predict = model.predict(x_test)
y_predict

model.score(x_train,y_train)

model.score(x_test,y_test)

np.sqrt(mean_squared_error(y_test,y_predict))

#print(y_test.head(10))





#-----------------------------------------------------------------------------------------------------------------------------------------------------------
Practical 6
Data Analytics II 
1. Implement logistic regression using Python/R to perform classification on Social_Network_Ads.csv dataset. 
2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall on the given dataset. 


#-----------------------------------------------------------------------------------------------------------------------------------------------------------




import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score


df = pd.read_csv("Social_Network_Ads.csv")
df


df['Gender'] = df['Gender'].replace({"Male":0 , "Female":1})
df

df.columns


x = df[['User ID', 'Gender', 'Age', 'EstimatedSalary', 'Purchased']]
y = df[['Purchased']]

x = x.fillna(x.mean())
y = y.fillna(y.mean())

x_train,x_test,y_train,y_test = train_test_split( x , y , test_size = 0.25 , random_state = 52)


model = LogisticRegression()
model.fit(x_train,y_train)
y_predict = model.predict(x_test) 

y_predict 

model.score(x_train,y_train)
model.score(x,y)

cm = confusion_matrix(y_test,y_predict)
cm

TP = cm[0, 0]
TN = cm[1, 1]
FP = cm[0, 1]
FN = cm[1, 0]

accuracy_score = (TP + TN) / (TP + TN + FP + FN)
accuracy_score

error_rate = 1 - accuracy_score
error_rate

precision_score = TP / (TP + FP)
precision_score

recall_score = TP / (TP + FN)
recall_score






#-----------------------------------------------------------------------------------------------------------------------------------------------------------
Practical 7:
Data Analytics III 
1. Implement Simple Naïve Bayes classification algorithm using Python/R on iris.csv dataset. 
2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall on the given dataset. 


#-----------------------------------------------------------------------------------------------------------------------------------------------------------




import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

df = pd.read_csv('/content/Iris.csv')
df.shape

df.columns

x = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]
y = df[['Species']]

x_train , x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=42)

model = GaussianNB()
model.fit(x_train,y_train)

y_predict = model.predict(x_test)
y_predict

model.score(x_test,y_test)
model.score(x_train,y_train)


cm = confusion_matrix(y_test,y_predict)
cm

accuracy_score(y_test,y_predict)

error_rate = 1 - accuracy_score
error_rate

precision_score(y_test , y_predict)

recall_score(y_test,y_predict)

#-----------------------------------------------------------------------------------------------------------------------------------------------------------
Practical - 9 :
Text Analytics 
Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization. 
Practical - 10 :
Text Analytics 
Extract Sample document and apply following document preprocessing methods: Stemming and Lemmatization.

#-----------------------------------------------------------------------------------------------------------------------------------------------------------




import nltk
from nltk.tokenize import TreebankWordTokenizer
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer,WordNetLemmatizer

# Fix: Download the updated resource versions
nltk.download('averaged_perceptron_tagger_eng', force=True)
nltk.download('stopwords', force=True)
nltk.download('wordnet', force=True)
nltk.download('omw-1.4', force=True)

# Sample document
text = "Text analytics is the process of extracting meaning from text. It helps in understanding patterns and insights."

print("Original Text:\n", text)

# 1. Tokenization using Treebank tokenizer
tokenizer = TreebankWordTokenizer()
tokens = tokenizer.tokenize(text)
print("\n1. Tokens:\n", tokens)

# 2. POS Tagging
pos_tags = pos_tag(tokens)
print("\n2. POS Tags:\n", pos_tags)

# 3. Stop Words Removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print("\n3. After Stop Words Removal:\n", filtered_tokens)

# 4. Stemming
stemmer = PorterStemmer()
stemmed = [stemmer.stem(word) for word in filtered_tokens]
print("\n4. After Stemming:\n", stemmed)

# 5. Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]
print("\n5. After Lemmatization:\n", lemmatized)





#-----------------------------------------------------------------------------------------------------------------------------------------------------------
Practical - 11:
Data Visualization I 
1. Use the inbuilt dataset 'titanic'. The dataset contains 891 rows and contains information about the passengers who boarded the unfortunate Titanic ship. Use the Seaborn library to see if we can find any patterns in the data. 
2. Write a code to check how the price of the ticket (column name: 'fare') for each passenger is distributed by plotting a histogram. 


#-----------------------------------------------------------------------------------------------------------------------------------------------------------




# Import required libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the Titanic dataset
titanic = sns.load_dataset('titanic')

# Display first 5 rows
print("Sample data:")
print(titanic.head())


# Count of survivors by gender
sns.countplot(x='sex', hue='survived', data=titanic)
plt.title("Survival Count by Gender")
plt.xlabel("Gender")
plt.ylabel("Count")
plt.legend(title="Survived (1=Yes, 0=No)")
plt.show()

# Survival by class
sns.countplot(x='class', hue='survived', data=titanic)
plt.title("Survival by Passenger Class")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

# 2. Plot histogram for Fare distribution

sns.histplot(titanic['fare'], kde=True, bins=30, color='green')
plt.title("Distribution of Ticket Fare")
plt.xlabel("Fare")
plt.ylabel("Number of Passengers")
plt.show()






#-----------------------------------------------------------------------------------------------------------------------------------------------------------
Practical 12:
Data Visualization II 
1. Use the inbuilt dataset 'titanic' as used in the above problem. Plot a box plot for distribution of age with respect to each gender along with the information about whether they survived or not. (Column names : 'sex' and 'age') 
2. Write observations on the inference from the above statistics. 
#-----------------------------------------------------------------------------------------------------------------------------------------------------------



# Import required libraries
import seaborn as sns
import matplotlib.pyplot as plt

# Load the Titanic dataset
titanic = sns.load_dataset('titanic')

# Display first few rows to understand the structure
print("Sample data:")
print(titanic[['sex', 'age', 'survived']].head())


# Box plot: Age distribution by gender and survival

sns.boxplot(x='sex', y='age', hue='survived', data=titanic)
plt.title("Age Distribution by Gender and Survival Status")
plt.xlabel("Gender")
plt.ylabel("Age")
plt.legend(title="Survived (1=Yes, 0=No)")
plt.show()

